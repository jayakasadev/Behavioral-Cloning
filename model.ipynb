{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going to load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_folder = \"./data\"\n",
    "\n",
    "lines = []\n",
    "\n",
    "with open(data_folder + \"/driving_log.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "print(\"There are\", len(lines), \"measurements.\")\n",
    "print(\"The data is in the following format:\")\n",
    "print(\"\\tCenter Image | Left Image | Right Image | Steering | Throttle | Brake | Speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing:\n",
    "1. Modify the paths of the images in the csv file so that this model can run on an EC2 instance with a GPU for training.\n",
    "2. Crop all images so that only the road visible and not the sky.\n",
    "3. Increase sample size by flipping all images in the the training set. There are less than 60k images and less than 20k steering measurements in this training set. It would be prudent to create some more using diferent transformations.\n",
    "4. Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going to visualize the images before they are modified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def display_images(left, center, right, lm, cm, rm):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "    # Fine-tune figure; make subplots spaced out\n",
    "    f.subplots_adjust(wspace = .3)\n",
    "    f.set_size_inches(12, 5)\n",
    "\n",
    "    ax1.imshow(left)\n",
    "    ax1.set_title(\"Left: \" + str(lm) + \" degrees\")\n",
    "    ax2.imshow(center)\n",
    "    ax2.set_title(\"Center: \" + str(cm) + \" degrees\")\n",
    "    ax3.imshow(right)\n",
    "    ax3.set_title(\"Right: \" + str(rm) + \" degrees\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# TODO: convert these values to numpy arrays because that is the format that keras requires\n",
    "# images will hold the images\n",
    "images = []\n",
    "\n",
    "# measurements will hold the measured driving angle\n",
    "measurements = []\n",
    "\n",
    "# steering offset hyperparameter\n",
    "offset = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modifiying the paths of the images\n",
    "# the below code was for testing on a machine with no gpu\n",
    "# count = 0\n",
    "for line in lines:\n",
    "    # images are in the following order: center, left, right\n",
    "    for i in range(0, 3):\n",
    "        source = line[i].split(\"\\\\\")[-1]\n",
    "        source = data_folder + \"/IMG/\" + source\n",
    "        image = cv2.imread(source)\n",
    "        # the image is originally opened in BGR, gonna have to change it to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "    center = float(line[3])\n",
    "    left = center + offset\n",
    "    right = center - offset\n",
    "    measurements.append(center)\n",
    "    measurements.append(left)\n",
    "    measurements.append(right)\n",
    "    \n",
    "    # the below code was for testing on a machine with no gpu\n",
    "    # count += 1\n",
    "    # if(count == 10):\n",
    "        # break;\n",
    "print(len(images), \"images\")\n",
    "print(images[0].shape)\n",
    "print(len(measurements), \"measurements\")\n",
    "print(measurements[0])\n",
    "\n",
    "display_images(images[1], images[0], images[2], measurements[1], measurements[0], measurements[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cropping the images\n",
    "def crop_image(image):\n",
    "    cropped = image[55:140, 0:320]\n",
    "    return cropped\n",
    "\n",
    "for i in range(len(images)):\n",
    "    images[i] = crop_image(images[i])\n",
    "\n",
    "display_images(images[1], images[0], images[2], measurements[1], measurements[0], measurements[2])\n",
    "print(images[0].shape)\n",
    "image_shape = images[0].shape\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### going to convert the images and measurement lists into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# images = np.array(cropped)\n",
    "# measurements = np.array(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# going to flip all images here\n",
    "modified_images = []\n",
    "modified_measurements = []\n",
    "for image in images:\n",
    "    modified_images.append(cv2.flip(image, 1))\n",
    "\n",
    "for measurement in measurements: \n",
    "    modified_measurements.append(-1 * measurement)\n",
    "\n",
    "print(\"There are\", len(modified_images), \"modified images.\")\n",
    "print(\"There are\", len(modified_measurements), \"modified measurements.\")\n",
    "\n",
    "modified_images = np.array(modified_images)\n",
    "modified_measurements = np.array(modified_measurements)\n",
    "\n",
    "display_images(modified_images[1], modified_images[0], modified_images[2],modified_measurements[1], modified_measurements[0], modified_measurements[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = np.append(images, modified_images, axis=0)\n",
    "measurements = np.append(measurements, modified_measurements)\n",
    "\n",
    "print(\"There are\", len(images), \"modified images.\")\n",
    "print(\"There are\", len(measurements), \"modified measurements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have 2X as many images and 6X as many measurements in my training set than I had before. However, it would be better if I applied a few transformations to the images to prevent overfitting and promote better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random shift (in case, I need more data)\n",
    "According to Nvidia's paper \"End to End Learning for Self-Driving Cars\", we should consider the center of each lane to the \"ground truth\". Unfortunately, human drivers do not always drive in the center of the lane. \n",
    "So, we have to manually calibrate the lane center associated with each frame. We do this by transforming the original images to account for departures from the \"ground truth\". The steering label for transformed images is adjusted to one that would steer the vehicle back to the desired location and orientation in two seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "images, measurements = shuffle(images, measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "x = [range(len(measurements))]\n",
    "x = np.squeeze(np.asarray(x))\n",
    "y = np.asarray(measurements)\n",
    "plt.xlim(0,len(measurements)-1)\n",
    "plt.title('data distribution', fontsize=17)\n",
    "plt.xlabel('frame')\n",
    "plt.ylabel('steering angle')\n",
    "plt.plot(x,y, 'r', linewidth=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(measurements, bins= len(np.unique(measurements)), color= 'red', linewidth=0.1)\n",
    "plt.title('angle histogram', fontsize=17)\n",
    "plt.xlabel('steering angle')\n",
    "plt.ylabel('counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the model\n",
    "I'm modelling my pipeline after GoogLenet. This means that I will make use of inception modules in my pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge, merge\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# the following arrays define the number of filters for each part of an inception layer \n",
    "# for each of the inception layers\n",
    "inception_layer1 = [64, 96, 128, 16, 32, 32]\n",
    "inception_layer2 = [128, 128, 192, 32, 96, 64]\n",
    "inception_layer3 = [192, 96, 208, 16, 48, 64]\n",
    "inception_layer4 = [160, 112, 224, 24, 64, 64]\n",
    "inception_layer5 = [128, 128, 256, 24, 64, 64]\n",
    "inception_layer6 = [112, 144, 288, 32, 64, 64]\n",
    "inception_layer7 = [256, 160, 320, 32, 128, 128]\n",
    "inception_layer8 = [256, 160, 320, 32, 128, 128]\n",
    "inception_layer9 = [384, 192, 384, 48, 128, 128]\n",
    "\n",
    "# method to build the inception layers\n",
    "# 3x3 reduce and 5x5 reduce stand for the number of 1x1 filters in the reduce layer used\n",
    "# before the 3x3 and 5x5 filters\n",
    "# this method adds 2 to the overall height of the model\n",
    "def inception(input_value, num_filters):\n",
    "    \n",
    "    # 1x1 \n",
    "    part1 = Convolution2D(num_filters[0], 1, 1, border_mode='same', activation='relu')(input_value)\n",
    "    # 3x3 reduce\n",
    "    part2 = Convolution2D(num_filters[1], 1, 1, border_mode='same', activation='relu')(input_value)\n",
    "    # 3x3\n",
    "    part2 = Convolution2D(num_filters[2], 3, 3, border_mode='same', activation='relu')(part2)\n",
    "    # 5x5 reduce\n",
    "    part3 = Convolution2D(num_filters[3], 1, 1, border_mode='same', activation='relu')(input_value)\n",
    "    # 5x5\n",
    "    part3 = Convolution2D(num_filters[4], 5, 5, border_mode='same', activation='relu')(part3)\n",
    "    # maxpool\n",
    "    part4 = MaxPooling2D((3, 3), strides=(1, 1), border_mode='same')(input_value)\n",
    "    # pool projection\n",
    "    part4 = Convolution2D(num_filters[5], 1, 1, border_mode='same', activation='relu')(part4)\n",
    "\n",
    "    return merge([part1, part2, part3, part4], mode='concat', concat_axis=3)\n",
    "\n",
    "def inceptionSeq(input_value, num_filters):\n",
    "    \n",
    "    shape = (input_value)\n",
    "    \n",
    "    input_value = Input(shape = input_value)\n",
    "    \n",
    "    part1 = Sequential()\n",
    "    # 1x1 \n",
    "    part1.add(Convolution2D(num_filters[0], 1, 1, border_mode='same', activation='relu', \n",
    "                            input_shape = shape))\n",
    "    \n",
    "    part2 = Sequential()\n",
    "    # 3x3 reduce\n",
    "    part2.add(Convolution2D(num_filters[1], 1, 1, border_mode='same', activation='relu', \n",
    "                            input_shape = shape))\n",
    "    # 3x3\n",
    "    part2.add(Convolution2D(num_filters[2], 3, 3, border_mode='same', activation='relu'))\n",
    "    \n",
    "    part3 = Sequential()\n",
    "    # 5x5 reduce\n",
    "    part3.add(Convolution2D(num_filters[3], 1, 1, border_mode='same', activation='relu', \n",
    "                            input_shape = shape))\n",
    "    # 5x5\n",
    "    part3.add(Convolution2D(num_filters[4], 5, 5, border_mode='same', activation='relu'))\n",
    "    \n",
    "    part4 = Sequential()\n",
    "    # maxpool\n",
    "    part4.add(MaxPooling2D((3, 3), strides=(1, 1), border_mode='same', \n",
    "                            input_shape = shape))\n",
    "    # pool projection\n",
    "    part4.add(Convolution2D(num_filters[5], 1, 1, border_mode='same', activation='relu'))\n",
    "    \n",
    "    return Merge([part1, part2, part3, part4], mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Input\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.layers.core import Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "# method to build the pipeline\n",
    "def build_GoogLenet():\n",
    "    input_img = Input(shape=image_shape)\n",
    "    \n",
    "    # convolution: (7x7, stride = 2, 64 filters)\n",
    "    layer1 = Convolution2D(64,7,7,subsample=(2,2),border_mode='same',activation='relu')(input_img)\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid)\n",
    "    layer1 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')(layer1)\n",
    "    \n",
    "    # 3x3 reduce convolution: (1x1, stride = 1, 64 filters)\n",
    "    layer2 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(layer1)\n",
    "    # convolution: (3x3, stride = 1, 192 filters)\n",
    "    layer2 = Convolution2D(192, 3, 3, border_mode='same', activation='relu')(layer2)\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid)\n",
    "    layer2 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')(layer2)\n",
    "    \n",
    "    # inception module 1\n",
    "    layer3 = inception(layer2, inception_layer1)\n",
    "    \n",
    "    # inception module 2\n",
    "    layer4 = inception(layer3, inception_layer2)\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid)\n",
    "    layer4 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')(layer4)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer5 = inception(layer4, inception_layer3)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer6 = inception(layer5, inception_layer4)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer7 = inception(layer6, inception_layer5)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer8 = inception(layer7, inception_layer6)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer9 = inception(layer8, inception_layer7)\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid)\n",
    "    layer10 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')(layer9)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer11 = inception(layer10, inception_layer8)\n",
    "    \n",
    "    #inception module 3\n",
    "    layer12 = inception(layer11, inception_layer9)\n",
    "    \n",
    "    # average pooling: (7x7, stride = 1)\n",
    "    layer13 = AveragePooling2D(pool_size=(3, 3),strides=(1,1))(layer12)\n",
    "    \n",
    "    # dropout: 40%\n",
    "    layer14 = Dropout(0.4)(layer13)\n",
    "    \n",
    "    # Flatten\n",
    "    layer15 = Flatten()(layer14)\n",
    "    \n",
    "    # Dense layer\n",
    "    layer16 = Dense(1)(layer15)\n",
    "    \n",
    "    layer17 = Activation('softmax')(layer16)\n",
    "    \n",
    "    model = Model(input=input_img, output = layer17)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def build_GoogLenetSeq():\n",
    "    # input = (85, 320, 3)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # convolution: (7x7, stride = 2, 64 filters) output = (42, 160, 64)\n",
    "    model.add(Convolution2D(64,7,7,subsample=(2,2),border_mode='same',activation='relu', \n",
    "                            input_shape = image_shape))\n",
    "    # curr_shape = (43, 160, 64)\n",
    "    curr_shape = (math.ceil(image_shape[0]/2), math.ceil(image_shape[1]/2), 64)\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid) output = (21, 80, 64)\n",
    "    model.add(MaxPooling2D((3, 3), strides=(2, 2), border_mode='same'))\n",
    "    \n",
    "    # curr_shape = (22, 80, 64)\n",
    "    curr_shape = (math.ceil(curr_shape[0]/2), math.ceil(curr_shape[1]/2), 64)\n",
    "    \n",
    "    # 3x3 reduce convolution: (1x1, stride = 1, 64 filters) output = (21, 80, 64)\n",
    "    model.add(Convolution2D(64, 1, 1, border_mode='same', activation='relu'))\n",
    "    # convolution: (3x3, stride = 1, 192 filters) output = (21, 80, 192)\n",
    "    model.add(Convolution2D(192, 3, 3, border_mode='same', activation='relu'))\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid) output = (11, 40, 192)\n",
    "    maxpool1 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')\n",
    "    model.add(maxpool1)\n",
    "        \n",
    "    # curr_shape = (10, 40, 192)\n",
    "    curr_shape = (math.ceil(curr_shape[0]/2), math.ceil(curr_shape[1]/2), 192)\n",
    "    \n",
    "    # inception module 1: output = (11, 40, 256)\n",
    "    print(\"inception module 1\")\n",
    "    inception1 = inceptionSeq(curr_shape, inception_layer1)\n",
    "    print(type(inception1))\n",
    "    model.add(inception1)\n",
    "        \n",
    "    # curr_shape = (11, 40, 192)\n",
    "    curr_shape = (curr_shape[0], curr_shape[1], 192)\n",
    "    \n",
    "    # inception module 2: output = (10, 40, 480)\n",
    "    print(\"inception module 2\")\n",
    "    inception2 = inceptionSeq(curr_shape, inception_layer2)\n",
    "    model.add(inception2)\n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid) output = (5, 20, 256)\n",
    "    maxpool2 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')(inception2)\n",
    "    model.add(maxpool2)\n",
    "    \n",
    "    # curr_shape = (5, 20, 256)\n",
    "    curr_shape = (math.ceil(curr_shape[0]/2), math.ceil(curr_shape[1]/2), 256)\n",
    "    \n",
    "    #inception module 3: output = (5, 20, 512)\n",
    "    print(\"inception module 3\")\n",
    "    inception3 = inceptionSeq(curr_shape, inception_layer3)\n",
    "    model.add(inception3)\n",
    "    \n",
    "    #inception module 4: output = (5, 20, 512)\n",
    "    print(\"inception module 4\")\n",
    "    inception4 = inceptionSeq(inception3, inception_layer4)\n",
    "    model.add(inception4)\n",
    "    \n",
    "    #inception module 5: output = (5, 20, 512)\n",
    "    print(\"inception module 5\")\n",
    "    inception5 = inceptionSeq(inception4, inception_layer5)\n",
    "    model.add(inception5)\n",
    "    \n",
    "    #inception module 6: output = (5, 20, 528)\n",
    "    print(\"inception module 6\")\n",
    "    inception6 = inceptionSeq(inception5, inception_layer6)\n",
    "    model.add(inception6)\n",
    "    \n",
    "    #inception module 7: output = (5, 20, 832)\n",
    "    print(\"inception module 7\")\n",
    "    model.add(inceptionSeq(inception6, inception_layer7))\n",
    "    \n",
    "    \n",
    "    # maxpool: (3x3, stride = 2, border_mode = same or valid): output = (2, 10, 832)\n",
    "    maxpool3 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='same')\n",
    "    model.add(maxpool3)\n",
    "    \n",
    "    # curr_shape = (2, 10, 832)\n",
    "    curr_shape = (math.ceil(curr_shape[0]/2), math.ceil(curr_shape[1]/2), 832)\n",
    "    \n",
    "    #inception module 8: output = (2, 10, 832)\n",
    "    print(\"inception module 8\")\n",
    "    inception8 = inceptionSeq(curr_shape, inception_layer8)\n",
    "    model.add(inception8)\n",
    "    \n",
    "    #inception module 9: output = (5, 20, 1024)\n",
    "    print(\"inception module 9\")\n",
    "    model.add(inceptionSeq(inception8, inception_layer9))\n",
    "    \n",
    "    # average pooling: (5x5, stride = 1): output = (1, 4, 1024)\n",
    "    model.add(AveragePooling2D(pool_size=(3,3),strides=(1,1)))\n",
    "    \n",
    "    # dropout: 40%: output = (1, 4, 1024)\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Flatten: output = (1, 1, 4100)\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer: output = (1, 1, 1)\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Softmax: output = (1, 1, 1)\n",
    "    model.add(Activation('softmax'))\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_GoogLenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile('adam', 'mean_squared_error', ['accuracy'])\n",
    "history = model.fit(images, measurements, batch_size=256, nb_epoch=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
